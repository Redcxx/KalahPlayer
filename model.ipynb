{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from agents.model_agent import MancalaModel\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from game.mancalaenv import MancalaEnv\n",
    "from torch import optim\n",
    "import torch.distributions as dist\n",
    "import os\n",
    "\n",
    "from game.play import play\n",
    "from agents.model_agent import ModelAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "time_tag = re.sub(r'[^\\d]', '-', str(datetime.now().time()))\n",
    "log_dir = \"runs/\" + time_tag\n",
    "writer = SummaryWriter(log_dir)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "seed = 1234\n",
    "n_holes = 7\n",
    "lr = 0.0001\n",
    "hidden_size = 1024\n",
    "max_game_length = 100\n",
    "reward_discount = 0.99\n",
    "epochs = 500000\n",
    "print_interval = 2000\n",
    "writer_interval = 2000\n",
    "solved_win_rate = 0.99\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = MancalaModel(hidden_size=hidden_size, n_inputs=n_holes*2, n_outputs=n_holes)\n",
    "env = MancalaEnv()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# summary(model, [(1, n_holes*2), ((hidden_size,), (hidden_size,))], device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_random_move(side, env):\n",
    "    return np.random.choice(env.get_valid_moves(side), 1, replace=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def calculate_loss(rewards, log_probabilities, values):\n",
    "    discounted_rewards = []\n",
    "    accumulated_rewards = 0\n",
    "    for current_reward in rewards[::-1]:\n",
    "        accumulated_rewards = reward_discount * accumulated_rewards + current_reward\n",
    "        discounted_rewards.append(accumulated_rewards)\n",
    "\n",
    "    discounted_rewards = torch.tensor(discounted_rewards[::-1]).float().to(device)\n",
    "    normalized_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + eps)\n",
    "\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    for reward, log_probability, value in zip(normalized_rewards, log_probabilities, values):\n",
    "        policy_loss.append((reward - value) * -log_probability)\n",
    "        value = value.squeeze(0).squeeze(0)\n",
    "        value_loss.append(F.smooth_l1_loss(value, reward))\n",
    "\n",
    "    return torch.stack(policy_loss).sum() + 0.5 * torch.stack(value_loss).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "def record(total_loss, env, epoch, model_side):\n",
    "    writer.add_scalar('loss', total_loss, epoch)\n",
    "    match_record_dir = log_dir + '/_' + str(epoch) + '.txt'\n",
    "    torch.save(model.state_dict(), log_dir + '_' + str(epoch) + '.msd')\n",
    "    with open(match_record_dir, 'w') as file:\n",
    "        file.write(f'model side is: {model_side}' + os.linesep)\n",
    "        for side, move, score, board in env.history:\n",
    "            file.write(f'{side} moved {move}, score: {score}' + os.linesep)\n",
    "            file.write(board + os.linesep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from agents.simpleagent import SimpleAgent\n",
    "\n",
    "def evaluation(model, epoch):\n",
    "    wins = 0\n",
    "    n_games = 100\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    simple_agent = SimpleAgent()\n",
    "    model_agent = ModelAgent(hidden_size=hidden_size, model=model)\n",
    "    for _ in range(n_games):\n",
    "        winner = play(model_agent, simple_agent, max_game_length=max_game_length)\n",
    "        if winner == model_agent:\n",
    "            wins += 1\n",
    "    writer.add_scalar(f'wins against simple agent / {n_games} games', wins, epoch)\n",
    "    print(f'wins against simple agent: {wins} / {n_games}, {wins/n_games*100:.2f}% @ epoch={epoch}/{epochs}')\n",
    "    return wins/n_games"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def select_action(env, side, model, hidden):\n",
    "    x = ModelAgent.get_model_input(env, side).to(device)\n",
    "    distribution, value, hidden = model.train().to(device)(x, hidden)\n",
    "    outputs = dist.Categorical(distribution)\n",
    "    action = outputs.sample()\n",
    "    return outputs.log_prob(action), action.item() + 1, value, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def train_one_game(model: nn.Module, epoch):\n",
    "    game_finished = False\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    values = []\n",
    "    log_probabilities = []\n",
    "    game_step = 0\n",
    "    win_rate = -1\n",
    "    next_player = random.choice(['north', 'south'])\n",
    "    hx = torch.zeros((1, hidden_size), dtype=torch.float).to(device)\n",
    "    cx = torch.zeros((1, hidden_size), dtype=torch.float).to(device)\n",
    "    hidden = (hx, cx)\n",
    "    opponent = SimpleAgent()\n",
    "    while not game_finished:\n",
    "        if next_player == 'north':\n",
    "            # model move\n",
    "            log_prob, action, value, hidden = select_action(env, 'north', model, hidden)\n",
    "            next_player, reward, done = env.step('north', action)\n",
    "            rewards.append(reward)\n",
    "            log_probabilities.append(log_prob)\n",
    "            values.append(value)\n",
    "        else:\n",
    "            # opponent move\n",
    "            with torch.no_grad():\n",
    "                next_player, _, done = env.step('south', opponent.get_move(env, 'south'))\n",
    "\n",
    "        game_step += 1\n",
    "        if done or game_step > max_game_length:\n",
    "            game_finished = True\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = calculate_loss(rewards, log_probabilities, values)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % writer_interval == 0:\n",
    "        record(total_loss, env, epoch, 'north')\n",
    "        win_rate = evaluation(model, epoch)\n",
    "\n",
    "    return total_loss.detach(), win_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins against simple agent: 55 / 100, 55.00% @ epoch=0/500000\n",
      "i=       0 loss=6.086492 elapsed=0:00:07\n",
      "wins against simple agent: 50 / 100, 50.00% @ epoch=2000/500000\n",
      "i=    2000 loss=0.014216 elapsed=0:02:11\n",
      "wins against simple agent: 47 / 100, 47.00% @ epoch=4000/500000\n",
      "i=    4000 loss=0.005759 elapsed=0:04:17\n",
      "wins against simple agent: 57 / 100, 57.00% @ epoch=6000/500000\n",
      "i=    6000 loss=0.002096 elapsed=0:06:27\n",
      "wins against simple agent: 55 / 100, 55.00% @ epoch=8000/500000\n",
      "i=    8000 loss=0.002480 elapsed=0:08:34\n",
      "wins against simple agent: 46 / 100, 46.00% @ epoch=10000/500000\n",
      "i=   10000 loss=0.000817 elapsed=0:10:45\n",
      "wins against simple agent: 41 / 100, 41.00% @ epoch=12000/500000\n",
      "i=   12000 loss=0.000786 elapsed=0:12:53\n",
      "wins against simple agent: 47 / 100, 47.00% @ epoch=14000/500000\n",
      "i=   14000 loss=0.001557 elapsed=0:15:00\n",
      "wins against simple agent: 47 / 100, 47.00% @ epoch=16000/500000\n",
      "i=   16000 loss=0.000309 elapsed=0:17:10\n",
      "wins against simple agent: 53 / 100, 53.00% @ epoch=18000/500000\n",
      "i=   18000 loss=0.000070 elapsed=0:19:19\n",
      "wins against simple agent: 48 / 100, 48.00% @ epoch=20000/500000\n",
      "i=   20000 loss=0.000100 elapsed=0:21:30\n",
      "wins against simple agent: 58 / 100, 58.00% @ epoch=22000/500000\n",
      "i=   22000 loss=0.000139 elapsed=0:23:34\n",
      "wins against simple agent: 53 / 100, 53.00% @ epoch=24000/500000\n",
      "i=   24000 loss=0.000190 elapsed=0:25:35\n",
      "wins against simple agent: 50 / 100, 50.00% @ epoch=26000/500000\n",
      "i=   26000 loss=-0.943111 elapsed=0:27:44\n",
      "wins against simple agent: 54 / 100, 54.00% @ epoch=28000/500000\n",
      "i=   28000 loss=0.063827 elapsed=0:29:51\n",
      "wins against simple agent: 54 / 100, 54.00% @ epoch=30000/500000\n",
      "i=   30000 loss=0.057936 elapsed=0:32:00\n",
      "wins against simple agent: 43 / 100, 43.00% @ epoch=32000/500000\n",
      "i=   32000 loss=0.025439 elapsed=0:34:08\n",
      "wins against simple agent: 51 / 100, 51.00% @ epoch=34000/500000\n",
      "i=   34000 loss=0.019489 elapsed=0:36:16\n",
      "wins against simple agent: 55 / 100, 55.00% @ epoch=36000/500000\n",
      "i=   36000 loss=0.005734 elapsed=0:38:23\n",
      "wins against simple agent: 55 / 100, 55.00% @ epoch=38000/500000\n",
      "i=   38000 loss=0.041180 elapsed=0:40:30\n",
      "wins against simple agent: 53 / 100, 53.00% @ epoch=40000/500000\n",
      "i=   40000 loss=0.000417 elapsed=0:42:37\n",
      "wins against simple agent: 50 / 100, 50.00% @ epoch=42000/500000\n",
      "i=   42000 loss=0.000645 elapsed=0:44:45\n",
      "wins against simple agent: 54 / 100, 54.00% @ epoch=44000/500000\n",
      "i=   44000 loss=0.001199 elapsed=0:46:57\n",
      "wins against simple agent: 58 / 100, 58.00% @ epoch=46000/500000\n",
      "i=   46000 loss=-0.004666 elapsed=0:49:12\n",
      "wins against simple agent: 50 / 100, 50.00% @ epoch=48000/500000\n",
      "i=   48000 loss=-0.000574 elapsed=0:51:25\n",
      "wins against simple agent: 46 / 100, 46.00% @ epoch=50000/500000\n",
      "i=   50000 loss=0.039134 elapsed=0:53:35\n",
      "wins against simple agent: 51 / 100, 51.00% @ epoch=52000/500000\n",
      "i=   52000 loss=198.792114 elapsed=0:56:04\n",
      "wins against simple agent: 50 / 100, 50.00% @ epoch=54000/500000\n",
      "i=   54000 loss=-276.640228 elapsed=0:58:31\n",
      "wins against simple agent: 51 / 100, 51.00% @ epoch=56000/500000\n",
      "i=   56000 loss=220.651642 elapsed=1:01:20\n",
      "wins against simple agent: 50 / 100, 50.00% @ epoch=58000/500000\n",
      "i=   58000 loss=-995.427490 elapsed=1:03:52\n",
      "wins against simple agent: 53 / 100, 53.00% @ epoch=60000/500000\n",
      "i=   60000 loss=703.812622 elapsed=1:06:20\n",
      "wins against simple agent: 0 / 100, 0.00% @ epoch=62000/500000\n",
      "i=   62000 loss=1014.099487 elapsed=1:09:03\n",
      "wins against simple agent: 0 / 100, 0.00% @ epoch=64000/500000\n",
      "i=   64000 loss=-875.810913 elapsed=1:11:44\n",
      "wins against simple agent: 58 / 100, 58.00% @ epoch=66000/500000\n",
      "i=   66000 loss=1021.875610 elapsed=1:14:20\n",
      "wins against simple agent: 0 / 100, 0.00% @ epoch=68000/500000\n",
      "i=   68000 loss=-3585.989014 elapsed=1:17:33\n",
      "wins against simple agent: 48 / 100, 48.00% @ epoch=70000/500000\n",
      "i=   70000 loss=-4282.575195 elapsed=1:20:56\n",
      "wins against simple agent: 100 / 100, 100.00% @ epoch=72000/500000\n",
      "i=   72000 loss=-42.290039 elapsed=1:23:28\n",
      "Game solved with win rate: 1.0 >= 0.99\n",
      "Finished Training, total time take: 1:23:28\n"
     ]
    }
   ],
   "source": [
    "import time, datetime\n",
    "\n",
    "def get_time_elapsed(start):\n",
    "    return str(datetime.timedelta(seconds=int(time.time() - start)))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, epochs):\n",
    "    loss, win_rate = train_one_game(model, i)\n",
    "    time_elapsed = int(time.time() - start_time)\n",
    "    if i % print_interval == 0:\n",
    "        print(f'i={i:8d} loss={loss:6f} elapsed={get_time_elapsed(start_time)}')\n",
    "    if win_rate >= solved_win_rate:\n",
    "        print(f'Game solved with win rate: {win_rate} >= {solved_win_rate}')\n",
    "        break\n",
    "print(f'Finished Training, total time take: {get_time_elapsed(start_time)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_model.msd')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_model.msd')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}