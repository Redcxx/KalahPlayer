{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    name = layer.__class__.__name__\n",
    "    if name.find('Linear') != -1:\n",
    "        init.xavier_uniform_(layer.weight.data)\n",
    "        layer.bias.data.fill_(0)\n",
    "\n",
    "class MancalaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs=16, n_outputs=16):\n",
    "        super().__init__()\n",
    "\n",
    "        n_neurons = 512\n",
    "\n",
    "        self.linear1 = nn.Linear(n_inputs, n_neurons)\n",
    "        self.linear2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.linear3 = nn.Linear(n_neurons, n_neurons)\n",
    "\n",
    "        self.actor = nn.Linear(n_neurons, n_outputs)\n",
    "        self.critics = nn.Linear(n_neurons, 1)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.linear1(x)), p=0.1)\n",
    "        x = F.dropout(F.relu(self.linear2(x)), p=0.1)\n",
    "        x = F.relu(self.linear3(x))\n",
    "        return F.softmax(self.actor(x), -1), self.critics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 512]           8,704\n",
      "            Linear-2               [-1, 1, 512]         262,656\n",
      "            Linear-3               [-1, 1, 512]         262,656\n",
      "            Linear-4                [-1, 1, 16]           8,208\n",
      "            Linear-5                 [-1, 1, 1]             513\n",
      "================================================================\n",
      "Total params: 542,737\n",
      "Trainable params: 542,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 2.07\n",
      "Estimated Total Size (MB): 2.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = MancalaModel()\n",
    "summary(model, (1, 16), device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "log_dir = \"runs/init_\" + re.sub(r'[^\\d]', '-', str(datetime.now().time()))\n",
    "writer = SummaryWriter(log_dir)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "seed = 313\n",
    "n_holes = 7\n",
    "lr = 0.01\n",
    "max_game_length = 100\n",
    "reward_discount = 0.9\n",
    "eps = 1e-7\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from game.mancalaenv import MancalaEnv\n",
    "from torch import optim\n",
    "\n",
    "env = MancalaEnv()\n",
    "model = MancalaModel(n_inputs=n_holes*2+2, n_outputs=n_holes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "def select_action(model_output):\n",
    "    outputs = dist.Categorical(model_output)\n",
    "    action = outputs.sample()\n",
    "    return outputs.log_prob(action), action.item() + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_random_move(side, env):\n",
    "    return np.random.choice(env.get_valid_moves(side), 1, replace=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_losses(rewards, log_probabilities, values):\n",
    "    discounted_rewards = []\n",
    "    accumulated_rewards = 0\n",
    "    for current_reward in rewards[::-1]:\n",
    "        accumulated_rewards = reward_discount * accumulated_rewards + current_reward\n",
    "        discounted_rewards.append(accumulated_rewards)\n",
    "\n",
    "    discounted_rewards = discounted_rewards[::-1]\n",
    "    discounted_rewards = torch.tensor(discounted_rewards).float().to(device)\n",
    "    normalized_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + eps)\n",
    "\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    for reward, log_probability, value in zip(normalized_rewards, log_probabilities, values):\n",
    "        policy_loss.append((reward - value.item()) * -log_probability)\n",
    "        reward = reward.unsqueeze(0).unsqueeze(0)\n",
    "        # print(\"value\", value.shape)\n",
    "        # print(\"value\", value)\n",
    "        # print(\"reward\", reward.shape)\n",
    "        # print(\"reward\", reward)\n",
    "        value_loss.append(F.smooth_l1_loss(value, reward.to(device)))\n",
    "\n",
    "    return policy_loss, value_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train_one_game(model: nn.Module):\n",
    "    game_finished = False\n",
    "    env.reset()\n",
    "    model.to(device)\n",
    "    rewards = []\n",
    "    values = []\n",
    "    log_probabilities = []\n",
    "\n",
    "    while not game_finished:\n",
    "        # print(env)\n",
    "        x = torch.unsqueeze(torch.from_numpy(env.board), 0).float().to(device)\n",
    "        # print(\"x\", x)\n",
    "        distribution, value = model(x)\n",
    "        # print(\"distribution\", distribution)\n",
    "        # print(\"value\", value)\n",
    "        log_prob, hole = select_action(distribution)\n",
    "        # print(\"hole\", hole)\n",
    "        # print(\"log_prob\", log_prob)\n",
    "        # print(\"action.item()\", action.item())\n",
    "        _, reward, done = env.step('north', hole)\n",
    "        model.train()\n",
    "\n",
    "        # opponent move\n",
    "        env.step('south', get_random_move('south', env))\n",
    "        # print(env)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        log_probabilities.append(log_prob)\n",
    "        values.append(value)\n",
    "\n",
    "        if done:\n",
    "            game_finished = True\n",
    "\n",
    "    # print(\"rewards\", rewards)\n",
    "    # print(\"log_probabilities\", log_probabilities)\n",
    "    # print(\"values\", values)\n",
    "    policy_loss, value_loss = get_losses(rewards, log_probabilities, values)\n",
    "    # print(\"p losses\", policy_loss)\n",
    "    # print(\"v losses\", value_loss)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=      5000 loss: -10.678265\n",
      "i=     10000 loss: 160.569931\n",
      "i=     15000 loss: 37.291012\n",
      "i=     20000 loss: 34.475227\n",
      "i=     25000 loss: 0.627771\n",
      "i=     30000 loss: 12.128546\n",
      "i=     35000 loss: 12.050107\n",
      "i=     40000 loss: 2.160525\n",
      "i=     45000 loss: 1.344607\n",
      "i=     50000 loss: 0.902831\n",
      "i=     55000 loss: 1.484973\n",
      "i=     60000 loss: 0.595250\n",
      "i=     65000 loss: 1.299363\n",
      "i=     70000 loss: 0.345010\n",
      "i=     75000 loss: 7.288711\n",
      "i=     80000 loss: 0.237803\n",
      "i=     85000 loss: 2.549199\n",
      "i=     90000 loss: 1.868187\n",
      "i=     95000 loss: 0.239661\n",
      "i=    100000 loss: 7.185636\n"
     ]
    }
   ],
   "source": [
    "interval = 5000\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    if i % interval == 0:\n",
    "        loss = train_one_game(model)\n",
    "        print(f'i={i:10d} loss: {loss:4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saved_models\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}