{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from agents.model_agent import MancalaModel\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from game.mancalaenv import MancalaEnv\n",
    "from torch import optim\n",
    "import torch.distributions as dist\n",
    "import os\n",
    "\n",
    "from agents.agent import Agent\n",
    "from game.mancala import Mancala\n",
    "from copy import deepcopy\n",
    "from game.play import play\n",
    "from agents.model_agent import ModelAgent\n",
    "\n",
    "import statistics as stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "time_tag = re.sub(r'[^\\d]', '-', str(datetime.now().time()))\n",
    "log_dir = \"runs/\" + time_tag\n",
    "writer = SummaryWriter(log_dir)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "seed = 1234\n",
    "n_holes = 7\n",
    "lr = 0.01\n",
    "max_game_length = 1000\n",
    "reward_discount = 0.99\n",
    "epochs = 200000\n",
    "print_interval = 2000\n",
    "writer_interval = 2000\n",
    "solved_win_rate = 0.99\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [1, 256]           3,840\n",
      "              ReLU-2                   [1, 256]               0\n",
      "            Linear-3                   [1, 256]          65,792\n",
      "              ReLU-4                   [1, 256]               0\n",
      "            Linear-5                     [1, 7]           1,799\n",
      "            Linear-6                     [1, 1]             257\n",
      "================================================================\n",
      "Total params: 71,688\n",
      "Trainable params: 71,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.27\n",
      "Estimated Total Size (MB): 0.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = MancalaModel(n_inputs=n_holes*2, n_outputs=n_holes)\n",
    "env = MancalaEnv()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "summary(model, (n_holes*2,), batch_size=1, device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_random_move(side, env):\n",
    "    return np.random.choice(env.get_valid_moves(side), 1, replace=False)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def calculate_loss(rewards, log_probabilities, values):\n",
    "    discounted_rewards = []\n",
    "    accumulated_rewards = 0\n",
    "    for current_reward in rewards[::-1]:\n",
    "        accumulated_rewards = reward_discount * accumulated_rewards + current_reward\n",
    "        discounted_rewards.append(accumulated_rewards)\n",
    "\n",
    "    discounted_rewards = torch.tensor(discounted_rewards[::-1]).float().to(device)\n",
    "    normalized_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + eps)\n",
    "\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    for reward, log_probability, value in zip(normalized_rewards, log_probabilities, values):\n",
    "        policy_loss.append((reward - value) * -log_probability)\n",
    "        reward = reward.unsqueeze(0)\n",
    "        value_loss.append(F.smooth_l1_loss(value, reward))\n",
    "\n",
    "    return torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "def record(total_loss, env, epoch, model_side):\n",
    "    writer.add_scalar('loss', total_loss, epoch)\n",
    "    match_record_dir = log_dir + '/_' + str(epoch) + '.txt'\n",
    "    torch.save(model.state_dict(), log_dir + '_' + str(epoch) + '.msd')\n",
    "    with open(match_record_dir, 'w') as file:\n",
    "        file.write(f'model side is: {model_side}' + os.linesep)\n",
    "        for side, move, score, board in env.move_history:\n",
    "            file.write(f'{side} moved {move}, score: {score}' + os.linesep)\n",
    "            file.write(board + os.linesep)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def get_move(self, env, side):\n",
    "        game = Mancala(n_holes, n_holes, board=deepcopy(env.board))\n",
    "        return np.random.choice(game.get_valid_moves(side), 1, replace=False)[0]\n",
    "\n",
    "def evaluation(model, epoch):\n",
    "    wins = 0\n",
    "    n_games = 100\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    random_agent = RandomAgent()\n",
    "    model_agent = ModelAgent(model=model)\n",
    "    for _ in range(n_games):\n",
    "        winner = play(model_agent, random_agent)\n",
    "        if winner == model_agent:\n",
    "            wins += 1\n",
    "\n",
    "    writer.add_scalar(f'wins against random / {n_games} games', wins, epoch)\n",
    "    print(f'wins against random agent: {wins} / {n_games}, {wins/n_games*100:.2f}% @ epoch={epoch}')\n",
    "    return wins/n_games"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def select_action(env, side, model):\n",
    "    x = ModelAgent.get_model_input(env, side).to(device)\n",
    "    distribution, value = model.train().to(device)(x)\n",
    "    outputs = dist.Categorical(distribution)\n",
    "    action = outputs.sample()\n",
    "    return outputs.log_prob(action), action.item() + 1, value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def train_one_game(model: nn.Module, epoch):\n",
    "    game_finished = False\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    values = []\n",
    "    log_probabilities = []\n",
    "    game_step = 0\n",
    "    win_rate = -1\n",
    "    next_player = random.choice(['north', 'south'])\n",
    "    while not game_finished:\n",
    "        if next_player == 'north':\n",
    "            # model move\n",
    "            log_prob, action, value = select_action(env, 'north', model)\n",
    "            next_player, reward, done = env.step('north', action)\n",
    "            rewards.append(reward)\n",
    "            log_probabilities.append(log_prob)\n",
    "            values.append(value)\n",
    "        else:\n",
    "            # opponent move\n",
    "            with torch.no_grad():\n",
    "                next_player, _, done = env.step('south', get_random_move('south', env))\n",
    "\n",
    "        game_step += 1\n",
    "        if done or game_step > max_game_length:\n",
    "            game_finished = True\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = calculate_loss(rewards, log_probabilities, values)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % writer_interval == 0:\n",
    "        record(total_loss, env, epoch, 'north')\n",
    "        win_rate = evaluation(model, epoch)\n",
    "\n",
    "    return total_loss.detach(), win_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins against random agent: 0 / 100, 0.00% @ epoch=0\n",
      "i=       0 loss=8.066968 elapsed=0:00:01\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=2000\n",
      "i=    2000 loss=0.011399 elapsed=0:00:14\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=4000\n",
      "i=    4000 loss=0.118097 elapsed=0:00:28\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=6000\n",
      "i=    6000 loss=0.054873 elapsed=0:00:41\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=8000\n",
      "i=    8000 loss=0.272164 elapsed=0:00:56\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=10000\n",
      "i=   10000 loss=3.568330 elapsed=0:01:15\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=12000\n",
      "i=   12000 loss=0.001453 elapsed=0:01:28\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=14000\n",
      "i=   14000 loss=0.211788 elapsed=0:01:41\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=16000\n",
      "i=   16000 loss=0.012473 elapsed=0:01:57\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=18000\n",
      "i=   18000 loss=0.673434 elapsed=0:02:13\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=20000\n",
      "i=   20000 loss=0.422632 elapsed=0:02:27\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=22000\n",
      "i=   22000 loss=1.090775 elapsed=0:02:51\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=24000\n",
      "i=   24000 loss=0.479046 elapsed=0:03:08\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=26000\n",
      "i=   26000 loss=7.840046 elapsed=0:03:26\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=28000\n",
      "i=   28000 loss=0.073644 elapsed=0:03:43\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=30000\n",
      "i=   30000 loss=1.120975 elapsed=0:04:00\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=32000\n",
      "i=   32000 loss=0.410335 elapsed=0:04:16\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=34000\n",
      "i=   34000 loss=-3.086035 elapsed=0:04:33\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=36000\n",
      "i=   36000 loss=1.427272 elapsed=0:04:50\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=38000\n",
      "i=   38000 loss=0.084552 elapsed=0:05:07\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=40000\n",
      "i=   40000 loss=0.014642 elapsed=0:05:21\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=42000\n",
      "i=   42000 loss=0.066359 elapsed=0:05:33\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=44000\n",
      "i=   44000 loss=-4.680162 elapsed=0:05:47\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=46000\n",
      "i=   46000 loss=0.744350 elapsed=0:06:07\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=48000\n",
      "i=   48000 loss=2.258498 elapsed=0:06:23\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=50000\n",
      "i=   50000 loss=0.370250 elapsed=0:06:41\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=52000\n",
      "i=   52000 loss=-0.973728 elapsed=0:06:57\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=54000\n",
      "i=   54000 loss=10.184345 elapsed=0:07:15\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=56000\n",
      "i=   56000 loss=17.724497 elapsed=0:07:37\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=58000\n",
      "i=   58000 loss=0.070960 elapsed=0:07:51\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=60000\n",
      "i=   60000 loss=0.067353 elapsed=0:08:03\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=62000\n",
      "i=   62000 loss=0.038599 elapsed=0:08:15\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=64000\n",
      "i=   64000 loss=0.002892 elapsed=0:08:26\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=66000\n",
      "i=   66000 loss=0.523023 elapsed=0:08:42\n",
      "wins against random agent: 0 / 100, 0.00% @ epoch=68000\n",
      "i=   68000 loss=0.063498 elapsed=0:08:54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-bd07311e2917>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mstart_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwin_rate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_one_game\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m     \u001B[0mtime_elapsed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mprint_interval\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-9-b1429dc82809>\u001B[0m in \u001B[0;36mtrain_one_game\u001B[1;34m(model, epoch)\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[0mtotal_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcalculate_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrewards\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlog_probabilities\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalues\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m     \u001B[0mtotal_loss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\environments\\python\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[0;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[1;32m--> 221\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\environments\\python\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    125\u001B[0m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_tensor_or_tensors_to_tuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad_tensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 126\u001B[1;33m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_make_grads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    127\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mretain_graph\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    128\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\environments\\python\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads)\u001B[0m\n\u001B[0;32m     49\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m                     \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"grad can be implicitly created only for scalar outputs\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m                 \u001B[0mnew_grads\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmemory_format\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreserve_format\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     52\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m                 \u001B[0mnew_grads\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import time, datetime\n",
    "\n",
    "def get_time_elapsed(start):\n",
    "    return str(datetime.timedelta(seconds=int(time.time() - start)))\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, epochs):\n",
    "    loss, win_rate = train_one_game(model, i)\n",
    "    time_elapsed = int(time.time() - start_time)\n",
    "    if i % print_interval == 0:\n",
    "        print(f'i={i:8d} loss={loss:6f} elapsed={get_time_elapsed(start_time)}')\n",
    "    if win_rate >= solved_win_rate:\n",
    "        print(f'Game solved with win rate: {win_rate} >= {solved_win_rate}')\n",
    "        break\n",
    "print(f'Finished Training, total time take: {get_time_elapsed(start_time)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_model.msd')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}